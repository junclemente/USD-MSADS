{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm_performance(cm, rnd = 5):\n",
    "    \"\"\"\n",
    "    Calculate and return various performance metrics from a confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "    cm (numpy.ndarray): Confusion matrix as a 2x2 numpy array.\n",
    "                        The format should be:\n",
    "                        [[TN, FP],\n",
    "                         [FN, TP]]\n",
    "    rnd (int, optional): Number of decimal places to round the performance metrics. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A dataframe containing the calculated performance metrics:\n",
    "                      - 'Accuracy': Proportion of correct predictions.\n",
    "                      - 'Error rate': Proportion of incorrect predictions.\n",
    "                      - 'Sensitivity (Recall)': True positive rate, the proportion of actual positives correctly identified.\n",
    "                      - 'Specificity': True negative rate, the proportion of actual negatives correctly identified.\n",
    "                      - 'Precision': Proportion of positive identifications that were actually correct.\n",
    "                      - 'F1': Harmonic mean of precision and recall.\n",
    "                      - 'F2': Weighted harmonic mean of precision and recall with more weight on recall.\n",
    "                      - 'F0.5': Weighted harmonic mean of precision and recall with more weight on precision.\n",
    "\n",
    "    Example:\n",
    "    >>> from sklearn.metrics import confusion_matrix\n",
    "    >>> import numpy as np\n",
    "    >>> cm = np.array([[50, 10],\n",
    "                       [5,  35]])\n",
    "    >>> cm_performance(cm)\n",
    "    [[50 10]\n",
    "     [ 5 35]]\n",
    "    TN,  FP\n",
    "    FN, TP\n",
    "           Metric   Value\n",
    "    0      Accuracy  0.85000\n",
    "    1     Error rate  0.15000\n",
    "    2  Sensitivity (Recall)  0.87500\n",
    "    3    Specificity  0.83333\n",
    "    4      Precision  0.77778\n",
    "    5            F1  0.82353\n",
    "    6            F2  0.85969\n",
    "    7          F0.5  0.79167\n",
    "\n",
    "    Docstring generated by ChatGPT.\n",
    "    \"\"\"\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    accuracy = (TN + TP) / (TN + FP + FN + TP)\n",
    "    error_rate = 1 - accuracy\n",
    "    sensitivity_recall = TP / (FN + TP)\n",
    "    specificity = TN / (TN + FP)\n",
    "    precision = TP / (FP + TP)\n",
    "    f1 = (2 * precision * sensitivity_recall) / (precision + sensitivity_recall)\n",
    "    f2 = (5 * precision * sensitivity_recall) / ((4 * precision) + sensitivity_recall)\n",
    "    f05 = (1.25 * precision * sensitivity_recall) / ((0.25 * precision) + sensitivity_recall)\n",
    "\n",
    "    data = {\n",
    "        'Metric': ['Accuracy', 'Error rate', 'Sensitivity (Recall)', \n",
    "                   'Specificity', 'Precision', 'F1', 'F2', 'F0.5'],\n",
    "        'Value': [accuracy, error_rate, sensitivity_recall, specificity,\n",
    "                  precision, f1, f2, f05]\n",
    "    }\n",
    "\n",
    "    performance_df = pd.DataFrame(data)\n",
    "    print(cm)\n",
    "    print(\"TN,  FP\\nFN, TP\")\n",
    "    return(performance_df.round(rnd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_cm_metrics(cm, rnd=5):\n",
    "    \"\"\"\n",
    "    Calculate and return various performance metrics from a confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "    cm (numpy.ndarray): Confusion matrix as a numpy array of any size.\n",
    "    rnd (int, optional): Number of decimal places to round the performance metrics. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A dataframe containing the calculated performance metrics:\n",
    "                      - 'Accuracy': Proportion of correct predictions.\n",
    "                      - 'Error rate': Proportion of incorrect predictions.\n",
    "                      - 'Sensitivity (Recall)': True positive rate for each class.\n",
    "                      - 'Specificity': True negative rate for each class.\n",
    "                      - 'Precision': Proportion of positive identifications that were actually correct for each class.\n",
    "                      - 'F1': Harmonic mean of precision and recall for each class.\n",
    "                      - 'F2': Weighted harmonic mean of precision and recall with more weight on recall for each class.\n",
    "                      - 'F0.5': Weighted harmonic mean of precision and recall with more weight on precision for each class.\n",
    "\n",
    "    Example:\n",
    "    >>> from sklearn.metrics import confusion_matrix\n",
    "    >>> import numpy as np\n",
    "    >>> cm = np.array([[50, 10, 5],\n",
    "                       [5, 35, 5],\n",
    "                       [5, 10, 40]])\n",
    "    >>> cm_performance(cm)\n",
    "\n",
    "    Docstring generated by ChatGPT\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize lists to hold metrics for each class\n",
    "    classes = cm.shape[0]\n",
    "    metrics = ['Accuracy', 'Error rate', 'Sensitivity (Recall)', \n",
    "               'Specificity', 'Precision', 'F1', 'F2', 'F0.5']\n",
    "    performance_dict = {metric: [] for metric in metrics}\n",
    "\n",
    "    # Calculate metrics for each class\n",
    "    for i in range(classes):\n",
    "        TP = cm[i, i]\n",
    "        FN = np.sum(cm[i, :]) - TP\n",
    "        FP = np.sum(cm[:, i]) - TP\n",
    "        TN = np.sum(cm) - (TP + FP + FN)\n",
    "\n",
    "        accuracy = (TP + TN) / np.sum(cm)\n",
    "        error_rate = 1 - accuracy\n",
    "        sensitivity_recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "        specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "        precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "        f1 = (2 * precision * sensitivity_recall) / (precision + sensitivity_recall) if (precision + sensitivity_recall) != 0 else 0\n",
    "        f2 = (5 * precision * sensitivity_recall) / ((4 * precision) + sensitivity_recall) if ((4 * precision) + sensitivity_recall) != 0 else 0\n",
    "        f05 = (1.25 * precision * sensitivity_recall) / ((0.25 * precision) + sensitivity_recall) if ((0.25 * precision) + sensitivity_recall) != 0 else 0\n",
    "\n",
    "        performance_dict['Accuracy'].append(accuracy)\n",
    "        performance_dict['Error rate'].append(error_rate)\n",
    "        performance_dict['Sensitivity (Recall)'].append(sensitivity_recall)\n",
    "        performance_dict['Specificity'].append(specificity)\n",
    "        performance_dict['Precision'].append(precision)\n",
    "        performance_dict['F1'].append(f1)\n",
    "        performance_dict['F2'].append(f2)\n",
    "        performance_dict['F0.5'].append(f05)\n",
    "\n",
    "    # Convert to DataFrame with classes as columns\n",
    "    performance_df = pd.DataFrame(performance_dict, \n",
    "                                  index=[f'Class {i}' for i in range(classes)])\n",
    "    performance_df = performance_df.T.round(rnd)  # Transpose and round\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    return performance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[50 10  5]\n",
      " [ 5 35  5]\n",
      " [ 5 10 40]]\n",
      "                      Class 0  Class 1  Class 2\n",
      "Accuracy              0.84848  0.81818  0.84848\n",
      "Error rate            0.15152  0.18182  0.15152\n",
      "Sensitivity (Recall)  0.76923  0.77778  0.72727\n",
      "Specificity           0.90000  0.83333  0.90909\n",
      "Precision             0.83333  0.63636  0.80000\n",
      "F1                    0.80000  0.70000  0.76190\n",
      "F2                    0.78125  0.74468  0.74074\n",
      "F0.5                  0.81967  0.66038  0.78431\n"
     ]
    }
   ],
   "source": [
    "cm = np.array([[50, 10, 5], [5, 35, 5], [5, 10, 40]])\n",
    "print(multiclass_cm_metrics(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mambaML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
